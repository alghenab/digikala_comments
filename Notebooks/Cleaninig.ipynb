{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Cleaninig.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1_u0BRQtgc63tipFRbZD9b82LIsuzeZMA","authorship_tag":"ABX9TyP6V/jsOnricskpZxZ6iJO+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["### Reading Data"],"metadata":{"id":"O9ey_ZNocBvU"}},{"cell_type":"code","source":["import pandas as pd\n","\n","df = pd.read_csv('/content/Digikala-comments/Data/Data.csv')\n","df.head()"],"metadata":{"id":"Mhc-v2JybZoc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Cleaning Data"],"metadata":{"id":"PUSJPn_XcXQR"}},{"cell_type":"code","source":["# Null Values\n","df = df[~((df.title.isnull()) & (df.comment.isnull()))]\n","df = df.reset_index(drop=True)"],"metadata":{"id":"_3xdYpSncab0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data Types\n","def set_types(df):\n","    df.title = df.title.astype(str)\n","    df.comment = df.comment.astype(str)\n","    df.rate = df.rate.astype('int')\n","    return df\n","df = set_types(df)"],"metadata":{"id":"4P653EHWcksZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Managing NaNs\n","def replace_nan(entry):\n","    if entry == 'nan':\n","        return '#'\n","    return entry\n","df.title = df.title.apply(replace_nan)\n","df.comment = df.comment.apply(replace_nan)"],"metadata":{"id":"W184gOY_cyNu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Making our sentiment colmn\n","df['Phrase'] = df['title'] + ' ' + df['comment']\n","df['Sentiment'] = df['rate']\n","df.drop(columns=['title', 'verification_status', 'comment', 'rate'], inplace=True) \n","df.head(1)"],"metadata":{"id":"hOQWvc3Cc9FM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Duplicates\n","df.drop_duplicates(subset =['Phrase'], inplace = True)"],"metadata":{"id":"aTv4baj2k4Vy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### more cleaning"],"metadata":{"id":"kcWtJU_Lgrqt"}},{"cell_type":"code","source":["import numpy as np\n","from hazm import word_tokenize, stopwords_list, InformalLemmatizer\n","import re\n","lemma = InformalLemmatizer()\n","\n","# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n","def remove_emoji(string):\n","    emoji_pattern = re.compile(\"[\"\n","                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                           u\"\\U00002702-\\U000027B0\"\n","                           u\"\\U000024C2-\\U0001F251\"\n","                           \"]+\", flags=re.UNICODE)\n","    return emoji_pattern.sub(r'', string)\n","\n","# https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py  emoticons list\n","# https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt Chat shortcuts\n","\n","def remove_urls(text):\n","    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n","    return url_pattern.sub(r' ', text)\n","\n","def remove_punctuations(text):\n","    punctuations = re.compile(r'[~`!@#$%^&*(,<،>){}\\\\/|\\'\"?؟_+-=~\\[\\]]')\n","    return punctuations.sub(r' ', text)\n","\n","def remove_html(text):\n","    html_pattern = re.compile('<.*?>')\n","    return html_pattern.sub(r' ', text)\n","\n","def remove_weird_chars(text):\n","    weridPatterns = re.compile(\"[\"\n","                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                               u\"\\U00002702-\\U000027B0\"\n","                               u\"\\U000024C2-\\U0001F251\"\n","                               u\"\\U0001f926-\\U0001f937\"\n","                               u'\\U00010000-\\U0010ffff'\n","                               u\"\\u200d\"\n","                               u\"\\u2640-\\u2642\"\n","                               u\"\\u2600-\\u2B55\"\n","                               u\"\\u23cf\"\n","                               u\"\\u23e9\"\n","                               u\"\\u231a\"\n","                               u\"\\u3030\"\n","                               u\"\\ufe0f\"\n","                               u\"\\u2069\"\n","                               u\"\\u2066\"\n","                               u\"\\u200c\"\n","                               u\"\\u2068\"\n","                               u\"\\u2067\"\n","                               \"]+\", flags=re.UNICODE)\n","    patterns = [re.compile('\\r'), re.compile('\\n'), re.compile('&amp;')]\n","    text = weridPatterns.sub(r'', text)\n","    for p in patterns:\n","        text = p.sub(r' ', text)\n","    return text\n","\n","def remove_extra_repeated_alpha(text):\n","    \"\"\"\n","    Remove extra repeated alphabets in a word\n","    check these links:\n","    demo : https://regex101.com/r/ALxocA/1\n","    Question: https://bit.ly/2DoiPqS\n","    \"\"\"\n","    return re.sub(r'([^\\W\\d_])\\1{2,}', r'\\1', text)\n","\n","\n","def clean_up(text, url=True, html=True, weird_patterns=True , lemmatize=False, \n","               stopwords=True, isalpha=False, punctuations=True, remove_extra_alpha=True):\n","    # remove url\n","    if url:\n","        text = remove_urls(text)\n","    # remove html tags\n","    if html:\n","        text = remove_html(text)\n","    # remove emokis / symbols & pictographs / transport & map symbols / flags (iOS)\n","    if weird_patterns:\n","        text = remove_weird_chars(text)\n","    # remove punctuations\n","    if punctuations:\n","        text = remove_punctuations(text)\n","    # Alter words with repeated alphabets\n","    if remove_extra_repeated_alpha:\n","        text = remove_extra_repeated_alpha(text)\n","    # tokenize text\n","    tokens = word_tokenize(text)\n","    # remove stop words\n","    if stopwords:\n","        tokens = [word for word in tokens if word not in stopwords_list()]\n","    # remove non-alphabetic items\n","    if isalpha:\n","        tokens = [word for word in tokens if word.isalpha()]\n","    # lemmatize words\n","    if lemmatize:\n","        tokens = [lemma.lemmatize(word) for word in tokens]\n","    text = ' '.join(tokens)\n","    \n","    return text\n","\n"],"metadata":{"id":"aIG6aCWzdcyx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# not cleaned example\n","test = df.loc[502].Phrase\n","test"],"metadata":{"id":"ppEAEEgSiobe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clean_up(test)"],"metadata":{"id":"f1s5jNeWlRkN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Applying our function to Phrases\n","# may take some time\n","# df['Phrase'] = df['Phrase'].apply(clean_up)"],"metadata":{"id":"4IE7U3NBgpcP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Saving the cleaned data\n","# df.to_csv('/content/drive/MyDrive/Digikala-comments/Data/Cleaned-data.csv')"],"metadata":{"id":"AXQOkf6wo2uS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Distribution of sentiment classes across our data.\n","df = pd.read_csv('/content/drive/MyDrive/Digikala-comments/Data/Cleaned-data.csv')\n","df.head()"],"metadata":{"id":"PPCamTakpoim"},"execution_count":null,"outputs":[]}]}